{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "A16_NeuralNetwork.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SF6FAoFijm0Q"
      },
      "source": [
        "#**Neural Network for Forest fires**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JSV1PfDiIM_M"
      },
      "source": [
        "## **Import Libraries**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nssQk6ByDu0Y",
        "outputId": "2d3cc942-b906-49fa-e0c4-e48513dc1649"
      },
      "source": [
        "!pip install keras\n",
        "!pip install tensorflow"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: keras in /usr/local/lib/python3.7/dist-packages (2.6.0)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.7/dist-packages (2.6.0)\n",
            "Requirement already satisfied: tensorflow-estimator~=2.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.6.0)\n",
            "Requirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.37.0)\n",
            "Requirement already satisfied: flatbuffers~=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.12)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.17.3)\n",
            "Requirement already satisfied: gast==0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.4.0)\n",
            "Requirement already satisfied: numpy~=1.19.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.19.5)\n",
            "Requirement already satisfied: tensorboard~=2.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.6.0)\n",
            "Requirement already satisfied: clang~=5.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (5.0)\n",
            "Requirement already satisfied: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.1.2)\n",
            "Requirement already satisfied: opt-einsum~=3.3.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: absl-py~=0.10 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.12.0)\n",
            "Requirement already satisfied: typing-extensions~=3.7.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.7.4.3)\n",
            "Requirement already satisfied: termcolor~=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.1.0)\n",
            "Requirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: keras~=2.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.6.0)\n",
            "Requirement already satisfied: six~=1.15.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.15.0)\n",
            "Requirement already satisfied: h5py~=3.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.1.0)\n",
            "Requirement already satisfied: astunparse~=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: wrapt~=1.12.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.12.1)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.37.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.41.0)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py~=3.1.0->tensorflow) (1.5.2)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow) (1.35.0)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow) (2.23.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow) (0.6.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow) (1.8.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow) (0.4.6)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow) (57.4.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow) (3.3.4)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow) (1.0.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.6->tensorflow) (4.7.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.6->tensorflow) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.6->tensorflow) (4.2.4)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard~=2.6->tensorflow) (4.8.1)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard~=2.6->tensorflow) (0.4.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (2021.5.30)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (1.24.3)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow) (3.1.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->markdown>=2.6.8->tensorboard~=2.6->tensorflow) (3.6.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xae4rX2YDuyu"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import keras\n",
        "from sklearn.preprocessing import StandardScaler"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DYMNjUeBaN8C"
      },
      "source": [
        "## **Data Collection and Description**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mfqhqqUVDuwY"
      },
      "source": [
        "df=pd.read_csv('forestfires.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 439
        },
        "id": "d-8omqnWDuvx",
        "outputId": "a0831d2a-19ab-489b-ce8a-f32be7532ec4"
      },
      "source": [
        "df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>month</th>\n",
              "      <th>day</th>\n",
              "      <th>FFMC</th>\n",
              "      <th>DMC</th>\n",
              "      <th>DC</th>\n",
              "      <th>ISI</th>\n",
              "      <th>temp</th>\n",
              "      <th>RH</th>\n",
              "      <th>wind</th>\n",
              "      <th>rain</th>\n",
              "      <th>area</th>\n",
              "      <th>dayfri</th>\n",
              "      <th>daymon</th>\n",
              "      <th>daysat</th>\n",
              "      <th>daysun</th>\n",
              "      <th>daythu</th>\n",
              "      <th>daytue</th>\n",
              "      <th>daywed</th>\n",
              "      <th>monthapr</th>\n",
              "      <th>monthaug</th>\n",
              "      <th>monthdec</th>\n",
              "      <th>monthfeb</th>\n",
              "      <th>monthjan</th>\n",
              "      <th>monthjul</th>\n",
              "      <th>monthjun</th>\n",
              "      <th>monthmar</th>\n",
              "      <th>monthmay</th>\n",
              "      <th>monthnov</th>\n",
              "      <th>monthoct</th>\n",
              "      <th>monthsep</th>\n",
              "      <th>size_category</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>mar</td>\n",
              "      <td>fri</td>\n",
              "      <td>86.2</td>\n",
              "      <td>26.2</td>\n",
              "      <td>94.3</td>\n",
              "      <td>5.1</td>\n",
              "      <td>8.2</td>\n",
              "      <td>51</td>\n",
              "      <td>6.7</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>small</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>oct</td>\n",
              "      <td>tue</td>\n",
              "      <td>90.6</td>\n",
              "      <td>35.4</td>\n",
              "      <td>669.1</td>\n",
              "      <td>6.7</td>\n",
              "      <td>18.0</td>\n",
              "      <td>33</td>\n",
              "      <td>0.9</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>small</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>oct</td>\n",
              "      <td>sat</td>\n",
              "      <td>90.6</td>\n",
              "      <td>43.7</td>\n",
              "      <td>686.9</td>\n",
              "      <td>6.7</td>\n",
              "      <td>14.6</td>\n",
              "      <td>33</td>\n",
              "      <td>1.3</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>small</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>mar</td>\n",
              "      <td>fri</td>\n",
              "      <td>91.7</td>\n",
              "      <td>33.3</td>\n",
              "      <td>77.5</td>\n",
              "      <td>9.0</td>\n",
              "      <td>8.3</td>\n",
              "      <td>97</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>small</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>mar</td>\n",
              "      <td>sun</td>\n",
              "      <td>89.3</td>\n",
              "      <td>51.3</td>\n",
              "      <td>102.2</td>\n",
              "      <td>9.6</td>\n",
              "      <td>11.4</td>\n",
              "      <td>99</td>\n",
              "      <td>1.8</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>small</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>512</th>\n",
              "      <td>aug</td>\n",
              "      <td>sun</td>\n",
              "      <td>81.6</td>\n",
              "      <td>56.7</td>\n",
              "      <td>665.6</td>\n",
              "      <td>1.9</td>\n",
              "      <td>27.8</td>\n",
              "      <td>32</td>\n",
              "      <td>2.7</td>\n",
              "      <td>0.0</td>\n",
              "      <td>6.44</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>large</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>513</th>\n",
              "      <td>aug</td>\n",
              "      <td>sun</td>\n",
              "      <td>81.6</td>\n",
              "      <td>56.7</td>\n",
              "      <td>665.6</td>\n",
              "      <td>1.9</td>\n",
              "      <td>21.9</td>\n",
              "      <td>71</td>\n",
              "      <td>5.8</td>\n",
              "      <td>0.0</td>\n",
              "      <td>54.29</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>large</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>514</th>\n",
              "      <td>aug</td>\n",
              "      <td>sun</td>\n",
              "      <td>81.6</td>\n",
              "      <td>56.7</td>\n",
              "      <td>665.6</td>\n",
              "      <td>1.9</td>\n",
              "      <td>21.2</td>\n",
              "      <td>70</td>\n",
              "      <td>6.7</td>\n",
              "      <td>0.0</td>\n",
              "      <td>11.16</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>large</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>515</th>\n",
              "      <td>aug</td>\n",
              "      <td>sat</td>\n",
              "      <td>94.4</td>\n",
              "      <td>146.0</td>\n",
              "      <td>614.7</td>\n",
              "      <td>11.3</td>\n",
              "      <td>25.6</td>\n",
              "      <td>42</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>small</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>516</th>\n",
              "      <td>nov</td>\n",
              "      <td>tue</td>\n",
              "      <td>79.5</td>\n",
              "      <td>3.0</td>\n",
              "      <td>106.7</td>\n",
              "      <td>1.1</td>\n",
              "      <td>11.8</td>\n",
              "      <td>31</td>\n",
              "      <td>4.5</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>small</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>517 rows Ã— 31 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "    month  day  FFMC    DMC  ...  monthnov  monthoct  monthsep  size_category\n",
              "0     mar  fri  86.2   26.2  ...         0         0         0          small\n",
              "1     oct  tue  90.6   35.4  ...         0         1         0          small\n",
              "2     oct  sat  90.6   43.7  ...         0         1         0          small\n",
              "3     mar  fri  91.7   33.3  ...         0         0         0          small\n",
              "4     mar  sun  89.3   51.3  ...         0         0         0          small\n",
              "..    ...  ...   ...    ...  ...       ...       ...       ...            ...\n",
              "512   aug  sun  81.6   56.7  ...         0         0         0          large\n",
              "513   aug  sun  81.6   56.7  ...         0         0         0          large\n",
              "514   aug  sun  81.6   56.7  ...         0         0         0          large\n",
              "515   aug  sat  94.4  146.0  ...         0         0         0          small\n",
              "516   nov  tue  79.5    3.0  ...         1         0         0          small\n",
              "\n",
              "[517 rows x 31 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2B_ARfnCDus-"
      },
      "source": [
        "df1=df.drop(['month','day'],axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7qVinUPoDupl"
      },
      "source": [
        "from sklearn.preprocessing import LabelEncoder"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7_EJ0fv_Duoo"
      },
      "source": [
        "lb=LabelEncoder()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aABhtXeEDulw"
      },
      "source": [
        "df1['size_category']=lb.fit_transform(df1['size_category'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iwL6lbCyDulB",
        "outputId": "f9cf4bfb-b11c-4895-ac51-c4fbbca2a3fb"
      },
      "source": [
        "df2=df1.values\n",
        "df2.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(517, 29)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cl-Q8qKjDuhX",
        "outputId": "aa03712a-0719-4ecb-f700-41bcdd65310b"
      },
      "source": [
        "x=df2[:,0:28]\n",
        "y=df2[:,-1]\n",
        "x.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(517, 28)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OXHSUXkladAc"
      },
      "source": [
        "## **Model Building**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aeCqL2M6DueY"
      },
      "source": [
        "from sklearn.model_selection import GridSearchCV, KFold\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pBMi9gweDudu"
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Dense(12, input_dim=28, activation='relu'))\n",
        "model.add(Dense(28, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YIGNb4VoDuah"
      },
      "source": [
        "model.compile(loss='binary_crossentropy', optimizer='Adam', metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GYRA18P0DuZu",
        "outputId": "753e5ee8-b99d-4f32-aff3-ba0308dff64c"
      },
      "source": [
        "model.fit(x, y, validation_split=0.33,epochs=100, batch_size=5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "70/70 [==============================] - 1s 4ms/step - loss: 0.5414 - accuracy: 0.7803 - val_loss: 0.6102 - val_accuracy: 0.7193\n",
            "Epoch 2/100\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.4479 - accuracy: 0.8092 - val_loss: 0.6107 - val_accuracy: 0.6140\n",
            "Epoch 3/100\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.4043 - accuracy: 0.8092 - val_loss: 0.5306 - val_accuracy: 0.7018\n",
            "Epoch 4/100\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.3496 - accuracy: 0.8439 - val_loss: 0.4054 - val_accuracy: 0.8421\n",
            "Epoch 5/100\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.3103 - accuracy: 0.8642 - val_loss: 0.3895 - val_accuracy: 0.8480\n",
            "Epoch 6/100\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.3015 - accuracy: 0.8728 - val_loss: 0.4658 - val_accuracy: 0.7368\n",
            "Epoch 7/100\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.2031 - accuracy: 0.9220 - val_loss: 0.3494 - val_accuracy: 0.8655\n",
            "Epoch 8/100\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.2002 - accuracy: 0.9249 - val_loss: 0.3482 - val_accuracy: 0.8772\n",
            "Epoch 9/100\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.1777 - accuracy: 0.9480 - val_loss: 0.2879 - val_accuracy: 0.8947\n",
            "Epoch 10/100\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.1515 - accuracy: 0.9509 - val_loss: 0.3108 - val_accuracy: 0.8830\n",
            "Epoch 11/100\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.1854 - accuracy: 0.9364 - val_loss: 0.2659 - val_accuracy: 0.8830\n",
            "Epoch 12/100\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.1545 - accuracy: 0.9451 - val_loss: 0.5357 - val_accuracy: 0.7310\n",
            "Epoch 13/100\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.1252 - accuracy: 0.9538 - val_loss: 0.2522 - val_accuracy: 0.8889\n",
            "Epoch 14/100\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.1065 - accuracy: 0.9538 - val_loss: 0.2034 - val_accuracy: 0.9064\n",
            "Epoch 15/100\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.1228 - accuracy: 0.9624 - val_loss: 0.2125 - val_accuracy: 0.9064\n",
            "Epoch 16/100\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.0948 - accuracy: 0.9624 - val_loss: 0.1801 - val_accuracy: 0.9181\n",
            "Epoch 17/100\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.1085 - accuracy: 0.9682 - val_loss: 0.6380 - val_accuracy: 0.7018\n",
            "Epoch 18/100\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.2057 - accuracy: 0.9162 - val_loss: 0.4398 - val_accuracy: 0.7895\n",
            "Epoch 19/100\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.0881 - accuracy: 0.9624 - val_loss: 0.1595 - val_accuracy: 0.9181\n",
            "Epoch 20/100\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.0603 - accuracy: 0.9798 - val_loss: 0.1828 - val_accuracy: 0.9064\n",
            "Epoch 21/100\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.0702 - accuracy: 0.9740 - val_loss: 0.3030 - val_accuracy: 0.8772\n",
            "Epoch 22/100\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.1136 - accuracy: 0.9624 - val_loss: 0.1397 - val_accuracy: 0.9474\n",
            "Epoch 23/100\n",
            "70/70 [==============================] - 0s 3ms/step - loss: 0.0785 - accuracy: 0.9682 - val_loss: 0.4564 - val_accuracy: 0.8713\n",
            "Epoch 24/100\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.0849 - accuracy: 0.9624 - val_loss: 0.1215 - val_accuracy: 0.9532\n",
            "Epoch 25/100\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.0620 - accuracy: 0.9711 - val_loss: 0.1295 - val_accuracy: 0.9474\n",
            "Epoch 26/100\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.0898 - accuracy: 0.9682 - val_loss: 0.1125 - val_accuracy: 0.9532\n",
            "Epoch 27/100\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.1055 - accuracy: 0.9595 - val_loss: 0.5944 - val_accuracy: 0.8596\n",
            "Epoch 28/100\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.1412 - accuracy: 0.9566 - val_loss: 0.1521 - val_accuracy: 0.9240\n",
            "Epoch 29/100\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.0505 - accuracy: 0.9798 - val_loss: 0.1345 - val_accuracy: 0.9357\n",
            "Epoch 30/100\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.0629 - accuracy: 0.9740 - val_loss: 0.3399 - val_accuracy: 0.8947\n",
            "Epoch 31/100\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.1354 - accuracy: 0.9480 - val_loss: 0.2445 - val_accuracy: 0.9064\n",
            "Epoch 32/100\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.0627 - accuracy: 0.9769 - val_loss: 0.0891 - val_accuracy: 0.9649\n",
            "Epoch 33/100\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.0793 - accuracy: 0.9711 - val_loss: 0.1049 - val_accuracy: 0.9649\n",
            "Epoch 34/100\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.0588 - accuracy: 0.9682 - val_loss: 0.2536 - val_accuracy: 0.9123\n",
            "Epoch 35/100\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.0470 - accuracy: 0.9855 - val_loss: 0.1167 - val_accuracy: 0.9591\n",
            "Epoch 36/100\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.0800 - accuracy: 0.9682 - val_loss: 0.1861 - val_accuracy: 0.9123\n",
            "Epoch 37/100\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.0681 - accuracy: 0.9740 - val_loss: 0.1031 - val_accuracy: 0.9532\n",
            "Epoch 38/100\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.1023 - accuracy: 0.9740 - val_loss: 0.0964 - val_accuracy: 0.9591\n",
            "Epoch 39/100\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.0376 - accuracy: 0.9884 - val_loss: 0.0971 - val_accuracy: 0.9591\n",
            "Epoch 40/100\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.0350 - accuracy: 0.9855 - val_loss: 0.0964 - val_accuracy: 0.9591\n",
            "Epoch 41/100\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.0406 - accuracy: 0.9884 - val_loss: 0.0925 - val_accuracy: 0.9532\n",
            "Epoch 42/100\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.0557 - accuracy: 0.9798 - val_loss: 0.1467 - val_accuracy: 0.9474\n",
            "Epoch 43/100\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.0328 - accuracy: 0.9884 - val_loss: 0.1028 - val_accuracy: 0.9649\n",
            "Epoch 44/100\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.0312 - accuracy: 0.9913 - val_loss: 0.0912 - val_accuracy: 0.9591\n",
            "Epoch 45/100\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.0379 - accuracy: 0.9798 - val_loss: 0.0816 - val_accuracy: 0.9708\n",
            "Epoch 46/100\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.0585 - accuracy: 0.9769 - val_loss: 0.0795 - val_accuracy: 0.9708\n",
            "Epoch 47/100\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.0298 - accuracy: 0.9884 - val_loss: 0.0936 - val_accuracy: 0.9649\n",
            "Epoch 48/100\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.0407 - accuracy: 0.9769 - val_loss: 0.0902 - val_accuracy: 0.9649\n",
            "Epoch 49/100\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.0566 - accuracy: 0.9711 - val_loss: 0.0875 - val_accuracy: 0.9474\n",
            "Epoch 50/100\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.0545 - accuracy: 0.9798 - val_loss: 0.0740 - val_accuracy: 0.9708\n",
            "Epoch 51/100\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.0390 - accuracy: 0.9827 - val_loss: 0.0881 - val_accuracy: 0.9474\n",
            "Epoch 52/100\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.0324 - accuracy: 0.9798 - val_loss: 0.1130 - val_accuracy: 0.9415\n",
            "Epoch 53/100\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.0507 - accuracy: 0.9855 - val_loss: 0.0964 - val_accuracy: 0.9415\n",
            "Epoch 54/100\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.0319 - accuracy: 0.9913 - val_loss: 0.1186 - val_accuracy: 0.9474\n",
            "Epoch 55/100\n",
            "70/70 [==============================] - 0s 3ms/step - loss: 0.0390 - accuracy: 0.9884 - val_loss: 0.1211 - val_accuracy: 0.9532\n",
            "Epoch 56/100\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.0444 - accuracy: 0.9855 - val_loss: 0.1782 - val_accuracy: 0.9298\n",
            "Epoch 57/100\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.0832 - accuracy: 0.9740 - val_loss: 0.0776 - val_accuracy: 0.9766\n",
            "Epoch 58/100\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.0364 - accuracy: 0.9827 - val_loss: 0.0953 - val_accuracy: 0.9532\n",
            "Epoch 59/100\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.1121 - accuracy: 0.9682 - val_loss: 0.0898 - val_accuracy: 0.9708\n",
            "Epoch 60/100\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.0344 - accuracy: 0.9827 - val_loss: 0.1795 - val_accuracy: 0.9298\n",
            "Epoch 61/100\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.0268 - accuracy: 0.9913 - val_loss: 0.0726 - val_accuracy: 0.9766\n",
            "Epoch 62/100\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.0423 - accuracy: 0.9884 - val_loss: 0.1371 - val_accuracy: 0.9415\n",
            "Epoch 63/100\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.0362 - accuracy: 0.9798 - val_loss: 0.4010 - val_accuracy: 0.8830\n",
            "Epoch 64/100\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.0315 - accuracy: 0.9884 - val_loss: 0.0765 - val_accuracy: 0.9532\n",
            "Epoch 65/100\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.0414 - accuracy: 0.9798 - val_loss: 0.1191 - val_accuracy: 0.9415\n",
            "Epoch 66/100\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.1278 - accuracy: 0.9595 - val_loss: 0.1274 - val_accuracy: 0.9532\n",
            "Epoch 67/100\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.0394 - accuracy: 0.9798 - val_loss: 0.1149 - val_accuracy: 0.9532\n",
            "Epoch 68/100\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.0455 - accuracy: 0.9884 - val_loss: 0.0775 - val_accuracy: 0.9532\n",
            "Epoch 69/100\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.0220 - accuracy: 0.9913 - val_loss: 0.0695 - val_accuracy: 0.9649\n",
            "Epoch 70/100\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.0362 - accuracy: 0.9884 - val_loss: 0.0826 - val_accuracy: 0.9649\n",
            "Epoch 71/100\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.0234 - accuracy: 0.9913 - val_loss: 0.2480 - val_accuracy: 0.9064\n",
            "Epoch 72/100\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.0782 - accuracy: 0.9653 - val_loss: 0.0685 - val_accuracy: 0.9825\n",
            "Epoch 73/100\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.0258 - accuracy: 0.9913 - val_loss: 0.1005 - val_accuracy: 0.9591\n",
            "Epoch 74/100\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.0214 - accuracy: 0.9942 - val_loss: 0.4022 - val_accuracy: 0.8830\n",
            "Epoch 75/100\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.0473 - accuracy: 0.9884 - val_loss: 0.0707 - val_accuracy: 0.9591\n",
            "Epoch 76/100\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.0348 - accuracy: 0.9855 - val_loss: 0.1264 - val_accuracy: 0.9591\n",
            "Epoch 77/100\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.0226 - accuracy: 0.9942 - val_loss: 0.0778 - val_accuracy: 0.9708\n",
            "Epoch 78/100\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.1684 - accuracy: 0.9624 - val_loss: 0.3326 - val_accuracy: 0.9123\n",
            "Epoch 79/100\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.0641 - accuracy: 0.9769 - val_loss: 0.1437 - val_accuracy: 0.9474\n",
            "Epoch 80/100\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.0201 - accuracy: 0.9971 - val_loss: 0.0810 - val_accuracy: 0.9532\n",
            "Epoch 81/100\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.0296 - accuracy: 0.9913 - val_loss: 0.0649 - val_accuracy: 0.9649\n",
            "Epoch 82/100\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.0225 - accuracy: 0.9913 - val_loss: 0.1035 - val_accuracy: 0.9649\n",
            "Epoch 83/100\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.0259 - accuracy: 0.9913 - val_loss: 0.0758 - val_accuracy: 0.9766\n",
            "Epoch 84/100\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.0232 - accuracy: 0.9942 - val_loss: 0.0927 - val_accuracy: 0.9591\n",
            "Epoch 85/100\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.0462 - accuracy: 0.9798 - val_loss: 0.1387 - val_accuracy: 0.9474\n",
            "Epoch 86/100\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.1043 - accuracy: 0.9711 - val_loss: 0.1574 - val_accuracy: 0.9415\n",
            "Epoch 87/100\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.0238 - accuracy: 0.9913 - val_loss: 0.1250 - val_accuracy: 0.9474\n",
            "Epoch 88/100\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.0160 - accuracy: 0.9942 - val_loss: 0.0720 - val_accuracy: 0.9649\n",
            "Epoch 89/100\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.0316 - accuracy: 0.9942 - val_loss: 0.1137 - val_accuracy: 0.9474\n",
            "Epoch 90/100\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.0363 - accuracy: 0.9769 - val_loss: 0.2432 - val_accuracy: 0.9240\n",
            "Epoch 91/100\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.0269 - accuracy: 0.9884 - val_loss: 0.3603 - val_accuracy: 0.9006\n",
            "Epoch 92/100\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.1002 - accuracy: 0.9711 - val_loss: 0.1043 - val_accuracy: 0.9591\n",
            "Epoch 93/100\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.0247 - accuracy: 0.9913 - val_loss: 0.0676 - val_accuracy: 0.9708\n",
            "Epoch 94/100\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.0208 - accuracy: 0.9913 - val_loss: 0.0732 - val_accuracy: 0.9591\n",
            "Epoch 95/100\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.0368 - accuracy: 0.9942 - val_loss: 0.0662 - val_accuracy: 0.9591\n",
            "Epoch 96/100\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.0193 - accuracy: 0.9884 - val_loss: 0.2073 - val_accuracy: 0.9415\n",
            "Epoch 97/100\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.0181 - accuracy: 0.9942 - val_loss: 0.0728 - val_accuracy: 0.9708\n",
            "Epoch 98/100\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.0581 - accuracy: 0.9798 - val_loss: 0.1297 - val_accuracy: 0.9532\n",
            "Epoch 99/100\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.0299 - accuracy: 0.9855 - val_loss: 0.0686 - val_accuracy: 0.9649\n",
            "Epoch 100/100\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.0327 - accuracy: 0.9855 - val_loss: 0.3187 - val_accuracy: 0.9006\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f5a8780de50>"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "faxi0GXjaqZb"
      },
      "source": [
        "##**Conclusion**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "obIrKhbzEqgG",
        "outputId": "2f56a4a6-8161-4b69-96e9-a581111b8e61"
      },
      "source": [
        "scores=model.evaluate(x,y)\n",
        "print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "17/17 [==============================] - 0s 1ms/step - loss: 0.1792 - accuracy: 0.9458\n",
            "accuracy: 94.58%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sR7V66vmFL19"
      },
      "source": [
        "# **Gas Turbine Neural Network Building**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FMpncwonawgh"
      },
      "source": [
        "##**Data Collection and Description**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N1D_cYc4EqaW"
      },
      "source": [
        "df=pd.read_csv(\"gas_turbines.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "u5LzyTWkEqZd",
        "outputId": "56ed8077-f9a8-4521-bef2-9e69510d7956"
      },
      "source": [
        "df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>AT</th>\n",
              "      <th>AP</th>\n",
              "      <th>AH</th>\n",
              "      <th>AFDP</th>\n",
              "      <th>GTEP</th>\n",
              "      <th>TIT</th>\n",
              "      <th>TAT</th>\n",
              "      <th>TEY</th>\n",
              "      <th>CDP</th>\n",
              "      <th>CO</th>\n",
              "      <th>NOX</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>6.8594</td>\n",
              "      <td>1007.9</td>\n",
              "      <td>96.799</td>\n",
              "      <td>3.5000</td>\n",
              "      <td>19.663</td>\n",
              "      <td>1059.2</td>\n",
              "      <td>550.00</td>\n",
              "      <td>114.70</td>\n",
              "      <td>10.605</td>\n",
              "      <td>3.1547</td>\n",
              "      <td>82.722</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>6.7850</td>\n",
              "      <td>1008.4</td>\n",
              "      <td>97.118</td>\n",
              "      <td>3.4998</td>\n",
              "      <td>19.728</td>\n",
              "      <td>1059.3</td>\n",
              "      <td>550.00</td>\n",
              "      <td>114.72</td>\n",
              "      <td>10.598</td>\n",
              "      <td>3.2363</td>\n",
              "      <td>82.776</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>6.8977</td>\n",
              "      <td>1008.8</td>\n",
              "      <td>95.939</td>\n",
              "      <td>3.4824</td>\n",
              "      <td>19.779</td>\n",
              "      <td>1059.4</td>\n",
              "      <td>549.87</td>\n",
              "      <td>114.71</td>\n",
              "      <td>10.601</td>\n",
              "      <td>3.2012</td>\n",
              "      <td>82.468</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>7.0569</td>\n",
              "      <td>1009.2</td>\n",
              "      <td>95.249</td>\n",
              "      <td>3.4805</td>\n",
              "      <td>19.792</td>\n",
              "      <td>1059.6</td>\n",
              "      <td>549.99</td>\n",
              "      <td>114.72</td>\n",
              "      <td>10.606</td>\n",
              "      <td>3.1923</td>\n",
              "      <td>82.670</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>7.3978</td>\n",
              "      <td>1009.7</td>\n",
              "      <td>95.150</td>\n",
              "      <td>3.4976</td>\n",
              "      <td>19.765</td>\n",
              "      <td>1059.7</td>\n",
              "      <td>549.98</td>\n",
              "      <td>114.72</td>\n",
              "      <td>10.612</td>\n",
              "      <td>3.2484</td>\n",
              "      <td>82.311</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15034</th>\n",
              "      <td>9.0301</td>\n",
              "      <td>1005.6</td>\n",
              "      <td>98.460</td>\n",
              "      <td>3.5421</td>\n",
              "      <td>19.164</td>\n",
              "      <td>1049.7</td>\n",
              "      <td>546.21</td>\n",
              "      <td>111.61</td>\n",
              "      <td>10.400</td>\n",
              "      <td>4.5186</td>\n",
              "      <td>79.559</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15035</th>\n",
              "      <td>7.8879</td>\n",
              "      <td>1005.9</td>\n",
              "      <td>99.093</td>\n",
              "      <td>3.5059</td>\n",
              "      <td>19.414</td>\n",
              "      <td>1046.3</td>\n",
              "      <td>543.22</td>\n",
              "      <td>111.78</td>\n",
              "      <td>10.433</td>\n",
              "      <td>4.8470</td>\n",
              "      <td>79.917</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15036</th>\n",
              "      <td>7.2647</td>\n",
              "      <td>1006.3</td>\n",
              "      <td>99.496</td>\n",
              "      <td>3.4770</td>\n",
              "      <td>19.530</td>\n",
              "      <td>1037.7</td>\n",
              "      <td>537.32</td>\n",
              "      <td>110.19</td>\n",
              "      <td>10.483</td>\n",
              "      <td>7.9632</td>\n",
              "      <td>90.912</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15037</th>\n",
              "      <td>7.0060</td>\n",
              "      <td>1006.8</td>\n",
              "      <td>99.008</td>\n",
              "      <td>3.4486</td>\n",
              "      <td>19.377</td>\n",
              "      <td>1043.2</td>\n",
              "      <td>541.24</td>\n",
              "      <td>110.74</td>\n",
              "      <td>10.533</td>\n",
              "      <td>6.2494</td>\n",
              "      <td>93.227</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15038</th>\n",
              "      <td>6.9279</td>\n",
              "      <td>1007.2</td>\n",
              "      <td>97.533</td>\n",
              "      <td>3.4275</td>\n",
              "      <td>19.306</td>\n",
              "      <td>1049.9</td>\n",
              "      <td>545.85</td>\n",
              "      <td>111.58</td>\n",
              "      <td>10.583</td>\n",
              "      <td>4.9816</td>\n",
              "      <td>92.498</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>15039 rows Ã— 11 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "           AT      AP      AH    AFDP  ...     TEY     CDP      CO     NOX\n",
              "0      6.8594  1007.9  96.799  3.5000  ...  114.70  10.605  3.1547  82.722\n",
              "1      6.7850  1008.4  97.118  3.4998  ...  114.72  10.598  3.2363  82.776\n",
              "2      6.8977  1008.8  95.939  3.4824  ...  114.71  10.601  3.2012  82.468\n",
              "3      7.0569  1009.2  95.249  3.4805  ...  114.72  10.606  3.1923  82.670\n",
              "4      7.3978  1009.7  95.150  3.4976  ...  114.72  10.612  3.2484  82.311\n",
              "...       ...     ...     ...     ...  ...     ...     ...     ...     ...\n",
              "15034  9.0301  1005.6  98.460  3.5421  ...  111.61  10.400  4.5186  79.559\n",
              "15035  7.8879  1005.9  99.093  3.5059  ...  111.78  10.433  4.8470  79.917\n",
              "15036  7.2647  1006.3  99.496  3.4770  ...  110.19  10.483  7.9632  90.912\n",
              "15037  7.0060  1006.8  99.008  3.4486  ...  110.74  10.533  6.2494  93.227\n",
              "15038  6.9279  1007.2  97.533  3.4275  ...  111.58  10.583  4.9816  92.498\n",
              "\n",
              "[15039 rows x 11 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HEzjhAMQFkLL",
        "outputId": "9af9642e-4be3-4114-a5f9-38b63da10187"
      },
      "source": [
        "df1=df.values\n",
        "df1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[   6.8594, 1007.9   ,   96.799 , ...,   10.605 ,    3.1547,\n",
              "          82.722 ],\n",
              "       [   6.785 , 1008.4   ,   97.118 , ...,   10.598 ,    3.2363,\n",
              "          82.776 ],\n",
              "       [   6.8977, 1008.8   ,   95.939 , ...,   10.601 ,    3.2012,\n",
              "          82.468 ],\n",
              "       ...,\n",
              "       [   7.2647, 1006.3   ,   99.496 , ...,   10.483 ,    7.9632,\n",
              "          90.912 ],\n",
              "       [   7.006 , 1006.8   ,   99.008 , ...,   10.533 ,    6.2494,\n",
              "          93.227 ],\n",
              "       [   6.9279, 1007.2   ,   97.533 , ...,   10.583 ,    4.9816,\n",
              "          92.498 ]])"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eze6gYIfa66O"
      },
      "source": [
        "##**Train And Test Data Split**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KkJGVgQhFkKe",
        "outputId": "98b26dc2-0aef-4c55-8836-102ad9862016"
      },
      "source": [
        "X=df1[:,[0,1,2,3,4,5,6,8,9,10]]\n",
        "Y=df1[:,-4]\n",
        "X"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[   6.8594, 1007.9   ,   96.799 , ...,   10.605 ,    3.1547,\n",
              "          82.722 ],\n",
              "       [   6.785 , 1008.4   ,   97.118 , ...,   10.598 ,    3.2363,\n",
              "          82.776 ],\n",
              "       [   6.8977, 1008.8   ,   95.939 , ...,   10.601 ,    3.2012,\n",
              "          82.468 ],\n",
              "       ...,\n",
              "       [   7.2647, 1006.3   ,   99.496 , ...,   10.483 ,    7.9632,\n",
              "          90.912 ],\n",
              "       [   7.006 , 1006.8   ,   99.008 , ...,   10.533 ,    6.2494,\n",
              "          93.227 ],\n",
              "       [   6.9279, 1007.2   ,   97.533 , ...,   10.583 ,    4.9816,\n",
              "          92.498 ]])"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nqqcddmwFkHF"
      },
      "source": [
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2g1qhUIfFkCY"
      },
      "source": [
        "x_train, x_test, y_train, y_test = train_test_split(X,Y,test_size=0.25,random_state=101)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5GDBJng8Fj9-"
      },
      "source": [
        "from sklearn.preprocessing import MinMaxScaler"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zkWZpFkDFj8O",
        "outputId": "2b85c3c7-e12c-4de6-b03a-fc5c0d793b4b"
      },
      "source": [
        "scaler=MinMaxScaler()\n",
        "scaler.fit(x_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MinMaxScaler(copy=True, feature_range=(0, 1))"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2LdpMVNkGC7H",
        "outputId": "171b3489-4799-4ca3-c346-1077d89d171f"
      },
      "source": [
        "x_train=scaler.transform(x_train)\n",
        "x_test=scaler.transform(x_test)\n",
        "x_test"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.35890393, 0.40602285, 0.91801706, ..., 0.34107329, 0.03084967,\n",
              "        0.48475958],\n",
              "       [0.55162803, 0.59086189, 0.72785444, ..., 0.42819611, 0.02833486,\n",
              "        0.43366477],\n",
              "       [0.69430373, 0.53478712, 0.55215014, ..., 0.14847583, 0.15186537,\n",
              "        0.33822331],\n",
              "       ...,\n",
              "       [0.29923532, 0.48494289, 0.94876603, ..., 0.77514199, 0.00101504,\n",
              "        0.41400706],\n",
              "       [0.64399376, 0.35825545, 0.50904718, ..., 0.04705791, 0.10100297,\n",
              "        0.36756316],\n",
              "       [0.3486443 , 0.24340602, 0.81637941, ..., 0.34416412, 0.00787964,\n",
              "        0.54170062]])"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UKUziQAUjH6u"
      },
      "source": [
        "##**Model Training and Building**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nhekVuokGC5l"
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Dense(10, activation='relu'))\n",
        "model.add(Dense(10, activation='relu'))\n",
        "model.add(Dense(10, activation='relu'))\n",
        "# add nodes for prediction\n",
        "model.add(Dense(1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d2bEt6ZGGC2M"
      },
      "source": [
        "model.compile(optimizer='rmsprop',loss='mse')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6d6DG8b5GCxl",
        "outputId": "14d2dc4f-d242-4521-ed36-490c290ac5a8"
      },
      "source": [
        "# Fit the model\n",
        "model.fit(x_train, y_train, epochs=250)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/250\n",
            "353/353 [==============================] - 1s 1ms/step - loss: 15283.0928\n",
            "Epoch 2/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 1384.8832\n",
            "Epoch 3/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 23.0965\n",
            "Epoch 4/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 18.7403\n",
            "Epoch 5/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 15.8723\n",
            "Epoch 6/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 13.6153\n",
            "Epoch 7/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 11.6324\n",
            "Epoch 8/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 9.9236\n",
            "Epoch 9/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 8.4341\n",
            "Epoch 10/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 7.0611\n",
            "Epoch 11/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 5.9509\n",
            "Epoch 12/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 4.8851\n",
            "Epoch 13/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 3.9990\n",
            "Epoch 14/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 3.2145\n",
            "Epoch 15/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 2.5873\n",
            "Epoch 16/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 2.1000\n",
            "Epoch 17/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 1.7197\n",
            "Epoch 18/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 1.4252\n",
            "Epoch 19/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 1.2316\n",
            "Epoch 20/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 1.0860\n",
            "Epoch 21/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 1.0012\n",
            "Epoch 22/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.9515\n",
            "Epoch 23/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.9161\n",
            "Epoch 24/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8959\n",
            "Epoch 25/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8741\n",
            "Epoch 26/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8631\n",
            "Epoch 27/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8665\n",
            "Epoch 28/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8509\n",
            "Epoch 29/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8500\n",
            "Epoch 30/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8477\n",
            "Epoch 31/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8517\n",
            "Epoch 32/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8509\n",
            "Epoch 33/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8614\n",
            "Epoch 34/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8392\n",
            "Epoch 35/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8480\n",
            "Epoch 36/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8551\n",
            "Epoch 37/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8487\n",
            "Epoch 38/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8528\n",
            "Epoch 39/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8438\n",
            "Epoch 40/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8541\n",
            "Epoch 41/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8390\n",
            "Epoch 42/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8295\n",
            "Epoch 43/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8490\n",
            "Epoch 44/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8446\n",
            "Epoch 45/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8444\n",
            "Epoch 46/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8365\n",
            "Epoch 47/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8455\n",
            "Epoch 48/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8459\n",
            "Epoch 49/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8436\n",
            "Epoch 50/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8338\n",
            "Epoch 51/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8420\n",
            "Epoch 52/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8450\n",
            "Epoch 53/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8323\n",
            "Epoch 54/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8297\n",
            "Epoch 55/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8270\n",
            "Epoch 56/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8403\n",
            "Epoch 57/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8353\n",
            "Epoch 58/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8390\n",
            "Epoch 59/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8428\n",
            "Epoch 60/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8304\n",
            "Epoch 61/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8341\n",
            "Epoch 62/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8392\n",
            "Epoch 63/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8329\n",
            "Epoch 64/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8322\n",
            "Epoch 65/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8144\n",
            "Epoch 66/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8515\n",
            "Epoch 67/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8362\n",
            "Epoch 68/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8146\n",
            "Epoch 69/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8315\n",
            "Epoch 70/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8394\n",
            "Epoch 71/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8212\n",
            "Epoch 72/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8144\n",
            "Epoch 73/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8239\n",
            "Epoch 74/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8349\n",
            "Epoch 75/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8220\n",
            "Epoch 76/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8122\n",
            "Epoch 77/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8188\n",
            "Epoch 78/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8202\n",
            "Epoch 79/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8174\n",
            "Epoch 80/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8105\n",
            "Epoch 81/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8116\n",
            "Epoch 82/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8215\n",
            "Epoch 83/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8238\n",
            "Epoch 84/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8153\n",
            "Epoch 85/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8123\n",
            "Epoch 86/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8159\n",
            "Epoch 87/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8238\n",
            "Epoch 88/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8100\n",
            "Epoch 89/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8225\n",
            "Epoch 90/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8186\n",
            "Epoch 91/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8213\n",
            "Epoch 92/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8166\n",
            "Epoch 93/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8154\n",
            "Epoch 94/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8144\n",
            "Epoch 95/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8252\n",
            "Epoch 96/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8226\n",
            "Epoch 97/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8158\n",
            "Epoch 98/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8103\n",
            "Epoch 99/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8079\n",
            "Epoch 100/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8291\n",
            "Epoch 101/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8152\n",
            "Epoch 102/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8081\n",
            "Epoch 103/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8166\n",
            "Epoch 104/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8157\n",
            "Epoch 105/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.7984\n",
            "Epoch 106/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8024\n",
            "Epoch 107/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8105\n",
            "Epoch 108/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8121\n",
            "Epoch 109/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8138\n",
            "Epoch 110/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8007\n",
            "Epoch 111/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8044\n",
            "Epoch 112/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8097\n",
            "Epoch 113/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8117\n",
            "Epoch 114/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8053\n",
            "Epoch 115/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8181\n",
            "Epoch 116/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.7884\n",
            "Epoch 117/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8052\n",
            "Epoch 118/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8019\n",
            "Epoch 119/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.7936\n",
            "Epoch 120/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8042\n",
            "Epoch 121/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.7985\n",
            "Epoch 122/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8038\n",
            "Epoch 123/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8053\n",
            "Epoch 124/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8133\n",
            "Epoch 125/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8062\n",
            "Epoch 126/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.7942\n",
            "Epoch 127/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8035\n",
            "Epoch 128/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8050\n",
            "Epoch 129/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.7946\n",
            "Epoch 130/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.7953\n",
            "Epoch 131/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.7882\n",
            "Epoch 132/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8029\n",
            "Epoch 133/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8075\n",
            "Epoch 134/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8104\n",
            "Epoch 135/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8017\n",
            "Epoch 136/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8022\n",
            "Epoch 137/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.7974\n",
            "Epoch 138/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8106\n",
            "Epoch 139/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.7828\n",
            "Epoch 140/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8073\n",
            "Epoch 141/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.7837\n",
            "Epoch 142/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.7956\n",
            "Epoch 143/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.7970\n",
            "Epoch 144/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.7990\n",
            "Epoch 145/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.7994\n",
            "Epoch 146/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.7806\n",
            "Epoch 147/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8014\n",
            "Epoch 148/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.7836\n",
            "Epoch 149/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8004\n",
            "Epoch 150/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.7935\n",
            "Epoch 151/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8088\n",
            "Epoch 152/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.7931\n",
            "Epoch 153/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.7899\n",
            "Epoch 154/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8043\n",
            "Epoch 155/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8006\n",
            "Epoch 156/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.7901\n",
            "Epoch 157/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8002\n",
            "Epoch 158/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.7947\n",
            "Epoch 159/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.7986\n",
            "Epoch 160/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.7953\n",
            "Epoch 161/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.7851\n",
            "Epoch 162/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.7766\n",
            "Epoch 163/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.7954\n",
            "Epoch 164/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.7905\n",
            "Epoch 165/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.7929\n",
            "Epoch 166/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.7914\n",
            "Epoch 167/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.7850\n",
            "Epoch 168/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.7771\n",
            "Epoch 169/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.7856\n",
            "Epoch 170/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.7934\n",
            "Epoch 171/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.7857\n",
            "Epoch 172/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.7861\n",
            "Epoch 173/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.7808\n",
            "Epoch 174/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.7968\n",
            "Epoch 175/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.7893\n",
            "Epoch 176/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.8002\n",
            "Epoch 177/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.7867\n",
            "Epoch 178/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.7697\n",
            "Epoch 179/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.7875\n",
            "Epoch 180/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.7846\n",
            "Epoch 181/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.7863\n",
            "Epoch 182/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.7802\n",
            "Epoch 183/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.7794\n",
            "Epoch 184/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.7871\n",
            "Epoch 185/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.7910\n",
            "Epoch 186/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.7889\n",
            "Epoch 187/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.7820\n",
            "Epoch 188/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.7860\n",
            "Epoch 189/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.7810\n",
            "Epoch 190/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.7723\n",
            "Epoch 191/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.7908\n",
            "Epoch 192/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.7802\n",
            "Epoch 193/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.7850\n",
            "Epoch 194/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.7819\n",
            "Epoch 195/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.7964\n",
            "Epoch 196/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.7763\n",
            "Epoch 197/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.7773\n",
            "Epoch 198/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.7865\n",
            "Epoch 199/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.7793\n",
            "Epoch 200/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.7779\n",
            "Epoch 201/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.7772\n",
            "Epoch 202/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.7799\n",
            "Epoch 203/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.7831\n",
            "Epoch 204/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.7699\n",
            "Epoch 205/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.7698\n",
            "Epoch 206/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.7690\n",
            "Epoch 207/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.7715\n",
            "Epoch 208/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.7755\n",
            "Epoch 209/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.7696\n",
            "Epoch 210/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.7858\n",
            "Epoch 211/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.7815\n",
            "Epoch 212/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.7728\n",
            "Epoch 213/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.7711\n",
            "Epoch 214/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.7816\n",
            "Epoch 215/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.7766\n",
            "Epoch 216/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.7612\n",
            "Epoch 217/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.7727\n",
            "Epoch 218/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.7704\n",
            "Epoch 219/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.7801\n",
            "Epoch 220/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.7719\n",
            "Epoch 221/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.7715\n",
            "Epoch 222/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.7801\n",
            "Epoch 223/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.7694\n",
            "Epoch 224/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.7635\n",
            "Epoch 225/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.7812\n",
            "Epoch 226/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.7779\n",
            "Epoch 227/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.7680\n",
            "Epoch 228/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.7634\n",
            "Epoch 229/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.7623\n",
            "Epoch 230/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.7686\n",
            "Epoch 231/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.7822\n",
            "Epoch 232/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.7748\n",
            "Epoch 233/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.7797\n",
            "Epoch 234/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.7678\n",
            "Epoch 235/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.7717\n",
            "Epoch 236/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.7797\n",
            "Epoch 237/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.7688\n",
            "Epoch 238/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.7672\n",
            "Epoch 239/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.7692\n",
            "Epoch 240/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.7648\n",
            "Epoch 241/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.7679\n",
            "Epoch 242/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.7742\n",
            "Epoch 243/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.7726\n",
            "Epoch 244/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.7697\n",
            "Epoch 245/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.7657\n",
            "Epoch 246/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.7604\n",
            "Epoch 247/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.7654\n",
            "Epoch 248/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.7703\n",
            "Epoch 249/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.7689\n",
            "Epoch 250/250\n",
            "353/353 [==============================] - 0s 1ms/step - loss: 0.7780\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f5a8383f350>"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        },
        "id": "tMEPdFMCGgIU",
        "outputId": "598a3efa-ad9c-4191-d6ec-9e07a408f131"
      },
      "source": [
        "%matplotlib inline\n",
        "import matplotlib as plot\n",
        "model_loss = pd.DataFrame(model.history.history)\n",
        "model_loss.plot()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f5a8372c610>"
            ]
          },
          "metadata": {},
          "execution_count": 51
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zZiW-AxXGgE9"
      },
      "source": [
        "pred = model.predict(x_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eXMXNQmkGgD3",
        "outputId": "5a55f7fb-f61f-4cc4-f411-936ca4a6849b"
      },
      "source": [
        "pred"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[133.43175 ],\n",
              "       [134.64279 ],\n",
              "       [111.701164],\n",
              "       ...,\n",
              "       [161.43932 ],\n",
              "       [105.141075],\n",
              "       [133.7095  ]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kXtkP1SBGgCc"
      },
      "source": [
        "pred = pred.ravel()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YUJFrRhBGf-8",
        "outputId": "3e54fea2-a9e6-441a-d401-6e74c652a157"
      },
      "source": [
        "test_score = model.evaluate(x_test,y_test,verbose=0)\n",
        "test_score"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.2034027576446533"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o3wwIl2BjWPF"
      },
      "source": [
        "##**Conclusion**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4aZ86y6YGzcy"
      },
      "source": [
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eb1Sx1YFGf7u"
      },
      "source": [
        "from sklearn.metrics import mean_absolute_error,mean_squared_error"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        },
        "id": "Nxgoozs8GzXc",
        "outputId": "b3664079-23df-4797-dc5e-e4f82ee5343b"
      },
      "source": [
        "plt.scatter(y_test,pred)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.collections.PathCollection at 0x7f5a835f7a90>"
            ]
          },
          "metadata": {},
          "execution_count": 60
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5npqyS6cGzlk",
        "outputId": "1283c058-25bc-4809-f68c-cf17e04e8484"
      },
      "source": [
        "mean_absolute_error(pred,y_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8973552770411714"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T5-zyZIDGzhG",
        "outputId": "c6e961f7-af33-4ed9-9257-6d9176c45bbc"
      },
      "source": [
        "mean_squared_error(pred,y_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.2034024956027916"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    }
  ]
}